{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox Notebook\n",
    "\n",
    "Experimental notebook for testing `home_media` package functionality.\n",
    "\n",
    "Auto-reload is enabled to pick up changes to the package without restarting the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable auto-reload for the home_media package\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Add parent directory to path so we can import home_media\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# Import home_media package\n",
    "from home_media import (\n",
    "    scan_directory,\n",
    "    list_subdirectories,\n",
    "    group_files_to_images,\n",
    "    extract_base_name,\n",
    "    extract_exif_metadata,\n",
    "    ExifData,\n",
    "    Image,\n",
    "    ImageFile,\n",
    "    FileRole,\n",
    "    FileFormat,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config from config.yaml\n",
    "config_path = Path.cwd().parent / \"config.yaml\"\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "photos_root = Path(config['photos_root_original'])\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"photos_root_original: {photos_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Subdirectories in photos_root_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use home_media to list subdirectories\n",
    "subdirs = list_subdirectories(photos_root)\n",
    "\n",
    "print(f\"Found {len(subdirs)} subdirectories in: {photos_root}\\n\")\n",
    "\n",
    "for subdir in subdirs[:20]:  # Show first 20\n",
    "    print(f\"  ðŸ“ {subdir.name}\")\n",
    "\n",
    "if len(subdirs) > 20:\n",
    "    print(f\"\\n  ... and {len(subdirs) - 20} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scan a Specific Subdirectory\n",
    "\n",
    "Let's examine a specific subdirectory to see what images and files we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a subdirectory to examine\n",
    "subdir = \"2025/01/01\"\n",
    "target_dir = photos_root / subdir\n",
    "\n",
    "print(f\"Scanning directory: {target_dir}\")\n",
    "print(f\"Relative path: {subdir}\")\n",
    "print()\n",
    "\n",
    "# Use home_media to scan and group files\n",
    "images_df, files_df = scan_directory(target_dir, photos_root=photos_root, extract_exif=True)\n",
    "\n",
    "print(f\"Found {len(images_df)} images with {len(files_df)} total files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images DataFrame\n",
    "\n",
    "One row per image (moment in time), with aggregated information about all its files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display images DataFrame\n",
    "print(\"Images DataFrame:\")\n",
    "print(f\"Columns: {list(images_df.columns)}\\n\")\n",
    "\n",
    "# Show first 10 images\n",
    "images_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"Image Statistics:\\n\")\n",
    "print(f\"Total images: {len(images_df)}\")\n",
    "print(f\"Images with RAW files: {images_df['has_raw'].sum()}\")\n",
    "print(f\"Images with JPEG files: {images_df['has_jpeg'].sum()}\")\n",
    "print(f\"Images with sidecars: {images_df['has_sidecar'].sum()}\")\n",
    "print(f\"\\nAverage files per image: {images_df['file_count'].mean():.1f}\")\n",
    "print(f\"Max files per image: {images_df['file_count'].max()}\")\n",
    "print(f\"\\nTotal storage: {images_df['total_size_bytes'].sum() / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show outlier images with the most files\n",
    "max_files = images_df['file_count'].max()\n",
    "outliers = images_df[images_df['file_count'] == max_files]\n",
    "\n",
    "print(f\"Images with {max_files} files (outliers):\\n\")\n",
    "for idx, row in outliers.iterrows():\n",
    "    print(f\"Base name: {row['base_name']}\")\n",
    "    print(f\"  Subdirectory: {row['subdirectory']}\")\n",
    "    print(f\"  File count: {row['file_count']}\")\n",
    "    print(f\"  Suffixes: {row['suffixes']}\")\n",
    "    print(f\"  Has RAW: {row['has_raw']}, Has JPEG: {row['has_jpeg']}, Has sidecar: {row['has_sidecar']}\")\n",
    "    print(f\"  Total size: {row['total_size_bytes'] / (1024**2):.2f} MB\")\n",
    "    print()\n",
    "\n",
    "# Also show top 5 images by file count\n",
    "print(\"\\nTop 5 images by file count:\")\n",
    "top_images = images_df.nlargest(5, 'file_count')[['base_name', 'subdirectory', 'file_count', 'suffixes', 'has_raw', 'has_jpeg', 'has_sidecar']]\n",
    "top_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files DataFrame\n",
    "\n",
    "One row per file, linked to images by `base_name` and `subdirectory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display files DataFrame\n",
    "print(\"Files DataFrame:\")\n",
    "print(f\"Columns: {list(files_df.columns)}\\n\")\n",
    "\n",
    "# Show first 10 files\n",
    "files_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File format breakdown\n",
    "print(\"File Format Distribution:\\n\")\n",
    "format_counts = files_df['format'].value_counts()\n",
    "for format_type, count in format_counts.items():\n",
    "    print(f\"  {format_type}: {count}\")\n",
    "\n",
    "print(\"\\nFile Role Distribution:\\n\")\n",
    "role_counts = files_df['role'].value_counts()\n",
    "for role, count in role_counts.items():\n",
    "    print(f\"  {role}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Image Objects\n",
    "\n",
    "For more complex operations, we can work with the `Image` objects directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file paths and group into Image objects\n",
    "file_paths = [f for f in target_dir.iterdir() if f.is_file()]\n",
    "images = group_files_to_images(file_paths, photos_root=photos_root)\n",
    "\n",
    "print(f\"Created {len(images)} Image objects\\n\")\n",
    "\n",
    "# Examine the first image in detail\n",
    "if images:\n",
    "    img = images[0]\n",
    "    print(f\"Image: {img.base_name}\")\n",
    "    print(f\"  Subdirectory: {img.subdirectory}\")\n",
    "    print(f\"  File count: {img.file_count}\")\n",
    "    print(f\"  Has RAW: {img.has_raw}\")\n",
    "    print(f\"  Has JPEG: {img.has_jpeg}\")\n",
    "    print(f\"  Has sidecar: {img.has_sidecar}\")\n",
    "    print(f\"  Total size: {img.total_size_bytes:,} bytes\")\n",
    "    print(f\"  Suffixes: {img.suffixes}\")\n",
    "    print(f\"\\n  Files:\")\n",
    "    for f in img.files:\n",
    "        print(f\"    - {f.filename}\")\n",
    "        print(f\"      Role: {f.role.name}, Format: {f.format.value}, Size: {f.file_size_bytes:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filename Pattern Analysis\n",
    "\n",
    "Let's see how the `extract_base_name()` function handles different filename patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different filename patterns\n",
    "test_filenames = [\n",
    "    \"2025-01-01_00-28-40.jpg\",\n",
    "    \"2025-01-01_00-28-40.CR3\",\n",
    "    \"2025-01-01_00-28-40_001.jpg\",\n",
    "    \"2025-01-01_00-28-40.jpg.xmp\",\n",
    "    \"PXL_20251210_200246684.RAW-01.COVER.jpg\",\n",
    "    \"PXL_20251210_200246684.RAW-02.ORIGINAL.dng\",\n",
    "    \"photo_edit.jpg\",\n",
    "]\n",
    "\n",
    "print(\"Filename Pattern Analysis:\\n\")\n",
    "for filename in test_filenames:\n",
    "    base_name, suffix = extract_base_name(filename)\n",
    "    print(f\"{filename}\")\n",
    "    print(f\"  base_name: {base_name}\")\n",
    "    print(f\"  suffix: {suffix}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images with Multiple Files\n",
    "\n",
    "Find images that have multiple files (RAW+JPEG pairs, sidecars, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter images with multiple files\n",
    "multi_file_images = images_df[images_df['file_count'] > 1]\n",
    "\n",
    "print(f\"Images with multiple files: {len(multi_file_images)} of {len(images_df)}\\n\")\n",
    "\n",
    "# Show examples\n",
    "print(\"Examples:\")\n",
    "for idx, row in multi_file_images.head(5).iterrows():\n",
    "    print(f\"\\n{row['base_name']}:\")\n",
    "    print(f\"  Files: {row['file_count']}\")\n",
    "    print(f\"  Suffixes: {row['suffixes']}\")\n",
    "    print(f\"  Has RAW: {row['has_raw']}, Has JPEG: {row['has_jpeg']}, Has sidecar: {row['has_sidecar']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canonical Naming Preview\n",
    "\n",
    "Preview how files would be renamed using the canonical naming scheme: `YYYY/mm/dd/YYYY-mm-dd_HH-MM-SS` + suffix.\n",
    "\n",
    "In the future, the capture time will be extracted from EXIF data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview canonical naming for the first image\n",
    "if images:\n",
    "    img = images[0]\n",
    "\n",
    "    # Simulate EXIF capture time (in real use, this comes from EXIF)\n",
    "    # Try to parse from the base_name if it follows our standard format\n",
    "    try:\n",
    "        # Try to parse datetime from base_name like \"2025-01-01_00-28-40\"\n",
    "        if len(img.base_name) >= 19 and img.base_name[10] == '_':\n",
    "            simulated_capture_time = datetime.strptime(img.base_name[:19], \"%Y-%m-%d_%H-%M-%S\")\n",
    "        else:\n",
    "            simulated_capture_time = datetime(2025, 1, 1, 0, 28, 40)\n",
    "    except Exception:\n",
    "        simulated_capture_time = datetime(2025, 1, 1, 0, 28, 40)\n",
    "\n",
    "    print(\"Current naming:\")\n",
    "    print(f\"  Base name: {img.base_name}\")\n",
    "    print(f\"  Subdirectory: {img.subdirectory}\")\n",
    "    print(f\"  Files: {img.file_count}\")\n",
    "\n",
    "    print(f\"\\nCanonical naming (based on capture time: {simulated_capture_time}):\")\n",
    "    print(f\"  Base name: {img.get_canonical_name(simulated_capture_time)}\")\n",
    "    print(f\"  Subdirectory: {img.get_canonical_subdirectory(simulated_capture_time)}\")\n",
    "\n",
    "    print(f\"\\nFull canonical paths would be:\")\n",
    "    for f in img.files:\n",
    "        canonical_path = (\n",
    "            f\"{img.get_canonical_subdirectory(simulated_capture_time)}/\"\n",
    "            f\"{img.get_canonical_name(simulated_capture_time)}{f.suffix}\"\n",
    "        )\n",
    "        print(f\"  {canonical_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Examples\n",
    "\n",
    "Use pandas to query the data in various ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find images that only have RAW files (no JPEG)\n",
    "raw_only = images_df[(images_df['has_raw']) & (~images_df['has_jpeg'])]\n",
    "print(f\"Images with only RAW (no JPEG): {len(raw_only)}\")\n",
    "raw_only.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find images with XMP sidecars\n",
    "with_sidecar = images_df[images_df['has_sidecar']]\n",
    "print(f\"Images with XMP sidecars: {len(with_sidecar)}\")\n",
    "with_sidecar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all CR3 (Canon RAW) files\n",
    "cr3_files = files_df[files_df['format'] == 'cr3']\n",
    "print(f\"Canon RAW (CR3) files: {len(cr3_files)}\")\n",
    "cr3_files.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Largest images by total size\n",
    "largest_images = images_df.nlargest(10, 'total_size_bytes')[['base_name', 'file_count', 'total_size_bytes']]\n",
    "largest_images['size_mb'] = largest_images['total_size_bytes'] / (1024**2)\n",
    "print(\"Largest images by total size:\\n\")\n",
    "largest_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXIF Metadata Extraction\n",
    "\n",
    "Test the new EXIF extraction functionality to populate Image metadata from the original files.\n",
    "\n",
    "**Supported formats:**\n",
    "- RAW files: CR2, CR3, NEF, ARW, DNG, RAF, ORF, RW2 (using exifread)\n",
    "- Standard formats: JPEG, PNG, TIFF (using Pillow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test EXIF extraction on a single image\n",
    "if images:\n",
    "    test_image = images[0]\n",
    "\n",
    "    print(f\"Testing EXIF extraction on: {test_image.base_name}\")\n",
    "    print(f\"Original file: {test_image.original_file.filename if test_image.original_file else 'None'}\")\n",
    "    print()\n",
    "\n",
    "    # Before extraction\n",
    "    print(\"Before EXIF extraction:\")\n",
    "    print(f\"  captured_at: {test_image.captured_at}\")\n",
    "    print(f\"  camera_make: {test_image.camera_make}\")\n",
    "    print(f\"  camera_model: {test_image.camera_model}\")\n",
    "    print(f\"  lens: {test_image.lens}\")\n",
    "    print(f\"  GPS: {test_image.gps_latitude}, {test_image.gps_longitude}\")\n",
    "    print()\n",
    "\n",
    "    # Extract EXIF\n",
    "    success = test_image.populate_from_exif()\n",
    "\n",
    "    # After extraction\n",
    "    print(f\"EXIF extraction {'successful' if success else 'failed'}\")\n",
    "    if success:\n",
    "        print(\"\\nAfter EXIF extraction:\")\n",
    "        print(f\"  captured_at: {test_image.captured_at}\")\n",
    "        print(f\"  camera_make: {test_image.camera_make}\")\n",
    "        print(f\"  camera_model: {test_image.camera_model}\")\n",
    "        print(f\"  lens: {test_image.lens}\")\n",
    "        print(f\"  GPS: {test_image.gps_latitude}, {test_image.gps_longitude}\")\n",
    "        print(f\"  title: {test_image.title}\")\n",
    "        print(f\"  description: {test_image.description}\")\n",
    "        print(f\"  rating: {test_image.rating}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test direct EXIF extraction from a file\n",
    "if images and test_image.original_file:\n",
    "    file_path = test_image.original_file.file_path\n",
    "    \n",
    "    print(f\"Direct EXIF extraction from: {file_path.name}\")\n",
    "    print()\n",
    "    \n",
    "    exif_data = extract_exif_metadata(file_path)\n",
    "    \n",
    "    if exif_data:\n",
    "        print(\"Extracted EXIF data:\")\n",
    "        print(f\"  captured_at: {exif_data.captured_at}\")\n",
    "        print(f\"  camera_make: {exif_data.camera_make}\")\n",
    "        print(f\"  camera_model: {exif_data.camera_model}\")\n",
    "        print(f\"  lens: {exif_data.lens}\")\n",
    "        print(f\"  GPS coordinates: ({exif_data.gps_latitude}, {exif_data.gps_longitude})\")\n",
    "        print(f\"  title: {exif_data.title}\")\n",
    "        print(f\"  description: {exif_data.description}\")\n",
    "        print(f\"  rating: {exif_data.rating}\")\n",
    "    else:\n",
    "        print(\"No EXIF data found or extraction failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scan Directory with EXIF Extraction\n",
    "\n",
    "Test the integrated EXIF extraction in `scan_directory()` - this will populate metadata for all images automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan directory with EXIF extraction enabled\n",
    "print(\"Scanning with EXIF extraction enabled...\")\n",
    "print(\"Note: This may be slower for large directories.\\n\")\n",
    "\n",
    "images_with_exif_df, files_with_exif_df = scan_directory(\n",
    "    target_dir, \n",
    "    photos_root=photos_root,\n",
    "    extract_exif=True\n",
    ")\n",
    "\n",
    "print(f\"Scanned {len(images_with_exif_df)} images\")\n",
    "print()\n",
    "\n",
    "# Show images with populated EXIF data\n",
    "print(\"Sample of images with EXIF data:\")\n",
    "exif_columns = ['base_name', 'captured_at', 'camera_make', 'camera_model']\n",
    "images_with_exif_df[exif_columns].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXIF data coverage statistics\n",
    "print(\"EXIF Data Coverage:\\n\")\n",
    "\n",
    "total_images = len(images_with_exif_df)\n",
    "with_capture_time = images_with_exif_df['captured_at'].notna().sum()\n",
    "with_camera_make = images_with_exif_df['camera_make'].notna().sum()\n",
    "with_camera_model = images_with_exif_df['camera_model'].notna().sum()\n",
    "\n",
    "print(f\"Total images: {total_images}\")\n",
    "print(f\"With capture time: {with_capture_time} ({with_capture_time/total_images*100:.1f}%)\")\n",
    "print(f\"With camera make: {with_camera_make} ({with_camera_make/total_images*100:.1f}%)\")\n",
    "print(f\"With camera model: {with_camera_model} ({with_camera_model/total_images*100:.1f}%)\")\n",
    "\n",
    "# Show unique cameras found\n",
    "if with_camera_make > 0:\n",
    "    print(\"\\nUnique cameras found:\")\n",
    "    camera_combos = images_with_exif_df[['camera_make', 'camera_model']].dropna()\n",
    "    unique_cameras = camera_combos.drop_duplicates()\n",
    "    for _, row in unique_cameras.iterrows():\n",
    "        print(f\"  {row['camera_make']} {row['camera_model']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare canonical naming with EXIF data vs filename-based\n",
    "if with_capture_time > 0:\n",
    "    # Get first image with EXIF capture time\n",
    "    sample = images_with_exif_df[images_with_exif_df['captured_at'].notna()].iloc[0]\n",
    "\n",
    "    print(\"Canonical naming comparison:\\n\")\n",
    "    print(f\"Original base_name: {sample['base_name']}\")\n",
    "    print(f\"Original subdirectory: {sample['subdirectory']}\")\n",
    "    print()\n",
    "    print(f\"EXIF captured_at: {sample['captured_at']}\")\n",
    "\n",
    "    # Create an Image object to use the canonical naming methods\n",
    "    temp_img = Image(\n",
    "        base_name=sample['base_name'],\n",
    "        subdirectory=sample['subdirectory'],\n",
    "        captured_at=sample['captured_at']\n",
    "    )\n",
    "\n",
    "    print(f\"\\nCanonical name (from EXIF): {temp_img.get_canonical_name()}\")\n",
    "    print(f\"Canonical subdirectory (from EXIF): {temp_img.get_canonical_subdirectory()}\")\n",
    "\n",
    "    # Show how files would be renamed\n",
    "    print(\"\\nFiles for this image would be renamed to:\")\n",
    "    sample_files = files_with_exif_df[\n",
    "        (files_with_exif_df['base_name'] == sample['base_name']) &\n",
    "        (files_with_exif_df['subdirectory'] == sample['subdirectory'])\n",
    "    ]\n",
    "\n",
    "    for _, file_row in sample_files.iterrows():\n",
    "        # Extract suffix from filename\n",
    "        suffix = file_row['filename'][len(sample['base_name']):]\n",
    "        canonical_path = f\"{temp_img.get_canonical_subdirectory()}/{temp_img.get_canonical_name()}{suffix}\"\n",
    "        print(f\"  {canonical_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scan with Full Metadata Extraction\n",
    "\n",
    "Test `scan_directory()` with all metadata options enabled: EXIF, file hashes, and dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan with full metadata extraction\n",
    "# Note: This will be slower due to hash calculation and dimension extraction\n",
    "print(\"Scanning with full metadata extraction...\")\n",
    "print(\"This includes: EXIF, file hashes (SHA256), and image dimensions\")\n",
    "print(\"Note: This may take a while for large directories.\\n\")\n",
    "\n",
    "images_full_df, files_full_df = scan_directory(\n",
    "    target_dir,\n",
    "    photos_root=photos_root,\n",
    "    extract_exif=True,\n",
    "    calculate_hash=True,\n",
    "    extract_dimensions=True\n",
    ")\n",
    "\n",
    "print(f\"Scanned {len(images_full_df)} images with {len(files_full_df)} files\")\n",
    "print()\n",
    "\n",
    "# Show sample with all metadata\n",
    "print(\"Sample of files with full metadata:\")\n",
    "metadata_columns = ['filename', 'format', 'role', 'width', 'height', 'file_size_bytes', 'file_hash']\n",
    "files_full_df[metadata_columns].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find potential duplicate files using hash\n",
    "duplicates = files_full_df[files_full_df['file_hash'].notna()].groupby('file_hash').filter(lambda x: len(x) > 1)\n",
    "\n",
    "if len(duplicates) > 0:\n",
    "    print(f\"Found {len(duplicates)} files with duplicate hashes:\\n\")\n",
    "    \n",
    "    # Group by hash and show duplicates\n",
    "    for hash_value, group in duplicates.groupby('file_hash'):\n",
    "        print(f\"Hash: {hash_value}\")\n",
    "        for _, file_row in group.iterrows():\n",
    "            print(f\"  - {file_row['filename']} ({file_row['file_size_bytes']:,} bytes)\")\n",
    "            print(f\"    Path: {file_row['file_path']}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No duplicate files found (based on hash)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata coverage statistics\n",
    "print(\"Metadata Coverage:\\n\")\n",
    "\n",
    "total_files = len(files_full_df)\n",
    "with_hash = files_full_df['file_hash'].notna().sum()\n",
    "with_dimensions = files_full_df['width'].notna().sum()\n",
    "\n",
    "print(f\"Total files: {total_files}\")\n",
    "print(f\"With hash: {with_hash} ({with_hash/total_files*100:.1f}%)\")\n",
    "print(f\"With dimensions: {with_dimensions} ({with_dimensions/total_files*100:.1f}%)\")\n",
    "\n",
    "# Show dimension statistics for image files only\n",
    "image_files = files_full_df[files_full_df['width'].notna()]\n",
    "if len(image_files) > 0:\n",
    "    print(\"\\nImage Dimension Statistics:\")\n",
    "    print(f\"  Width range: {image_files['width'].min()} - {image_files['width'].max()} pixels\")\n",
    "    print(f\"  Height range: {image_files['height'].min()} - {image_files['height'].max()} pixels\")\n",
    "    print(f\"  Average dimensions: {image_files['width'].mean():.0f}x{image_files['height'].mean():.0f} pixels\")\n",
    "    \n",
    "    # Calculate megapixels\n",
    "    image_files_copy = image_files.copy()\n",
    "    image_files_copy['megapixels'] = (image_files_copy['width'] * image_files_copy['height']) / 1_000_000\n",
    "    print(f\"  Average megapixels: {image_files_copy['megapixels'].mean():.2f} MP\")\n",
    "    print(f\"  Max megapixels: {image_files_copy['megapixels'].max():.2f} MP\")\n",
    "    \n",
    "    # Show unique resolutions\n",
    "    unique_resolutions = image_files[['width', 'height']].drop_duplicates().sort_values(['width', 'height'], ascending=False)\n",
    "    print(f\"\\nUnique resolutions found: {len(unique_resolutions)}\")\n",
    "    for _, row in unique_resolutions.head(10).iterrows():\n",
    "        mp = (row['width'] * row['height']) / 1_000_000\n",
    "        print(f\"  {int(row['width'])}x{int(row['height'])} ({mp:.2f} MP)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test hash and dimensions extraction on a single file\n",
    "if images:\n",
    "    test_image = images[0]\n",
    "    test_file = test_image.files[0] if test_image.files else None\n",
    "    \n",
    "    if test_file:\n",
    "        print(f\"Testing on file: {test_file.filename}\")\n",
    "        print(f\"Format: {test_file.format.value}, Role: {test_file.role.name}\")\n",
    "        print()\n",
    "        \n",
    "        # Before extraction\n",
    "        print(\"Before extraction:\")\n",
    "        print(f\"  file_hash: {test_file.file_hash}\")\n",
    "        print(f\"  dimensions: {test_file.width}x{test_file.height}\")\n",
    "        print()\n",
    "        \n",
    "        # Extract hash\n",
    "        print(\"Calculating SHA256 hash...\")\n",
    "        hash_success = test_file.populate_hash()\n",
    "        print(f\"  Success: {hash_success}\")\n",
    "        if hash_success:\n",
    "            print(f\"  Hash: {test_file.file_hash}\")\n",
    "        print()\n",
    "        \n",
    "        # Extract dimensions\n",
    "        print(\"Extracting dimensions...\")\n",
    "        dim_success = test_file.populate_dimensions()\n",
    "        print(f\"  Success: {dim_success}\")\n",
    "        if dim_success:\n",
    "            print(f\"  Dimensions: {test_file.width}x{test_file.height} pixels\")\n",
    "            aspect_ratio = test_file.width / test_file.height if test_file.height else 0\n",
    "            print(f\"  Aspect ratio: {aspect_ratio:.3f}\")\n",
    "            megapixels = (test_file.width * test_file.height) / 1_000_000 if test_file.width and test_file.height else 0\n",
    "            print(f\"  Megapixels: {megapixels:.2f} MP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Hash and Dimensions\n",
    "\n",
    "Test the new file hash and dimensions extraction functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata coverage statistics\n",
    "print(\"Metadata Coverage:\\n\")\n",
    "\n",
    "total_files = len(files_full_df)\n",
    "with_hash = files_full_df['file_hash'].notna().sum()\n",
    "with_dimensions = files_full_df['width'].notna().sum()\n",
    "\n",
    "print(f\"Total files: {total_files}\")\n",
    "print(f\"With hash: {with_hash} ({with_hash/total_files*100:.1f}%)\")\n",
    "print(f\"With dimensions: {with_dimensions} ({with_dimensions/total_files*100:.1f}%)\")\n",
    "\n",
    "# Show dimension statistics for image files only\n",
    "image_files = files_full_df[files_full_df['width'].notna()]\n",
    "if len(image_files) > 0:\n",
    "    print(\"\\nImage Dimension Statistics:\")\n",
    "    print(f\"  Width range: {image_files['width'].min()} - {image_files['width'].max()} pixels\")\n",
    "    print(f\"  Height range: {image_files['height'].min()} - {image_files['height'].max()} pixels\")\n",
    "    print(f\"  Average dimensions: {image_files['width'].mean():.0f}x{image_files['height'].mean():.0f} pixels\")\n",
    "    \n",
    "    # Calculate megapixels\n",
    "    image_files_copy = image_files.copy()\n",
    "    image_files_copy['megapixels'] = (image_files_copy['width'] * image_files_copy['height']) / 1_000_000\n",
    "    print(f\"  Average megapixels: {image_files_copy['megapixels'].mean():.2f} MP\")\n",
    "    print(f\"  Max megapixels: {image_files_copy['megapixels'].max():.2f} MP\")\n",
    "    \n",
    "    # Show unique resolutions\n",
    "    unique_resolutions = image_files[['width', 'height']].drop_duplicates().sort_values(['width', 'height'], ascending=False)\n",
    "    print(f\"\\nUnique resolutions found: {len(unique_resolutions)}\")\n",
    "    for _, row in unique_resolutions.head(10).iterrows():\n",
    "        mp = (row['width'] * row['height']) / 1_000_000\n",
    "        print(f\"  {int(row['width'])}x{int(row['height'])} ({mp:.2f} MP)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
