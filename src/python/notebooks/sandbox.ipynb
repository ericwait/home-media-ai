{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox Notebook\n",
    "\n",
    "Experimental notebook for testing `home_media` package functionality.\n",
    "\n",
    "Auto-reload is enabled to pick up changes to the package without restarting the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable auto-reload for the home_media package\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# Add parent directory to path so we can import home_media\n",
    "sys.path.insert(0, str(Path.cwd().parent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config from config.yaml\n",
    "config_path = Path.cwd().parent / \"config.yaml\"\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"photos_root_original: {config['photos_root_original']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Subdirectories in photos_root_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the photos root directory\n",
    "photos_root = Path(config['photos_root_original'])\n",
    "# photos_root = Path(r'\\\\tiger\\home\\Photos\\MobileBackup\\Pixel 8 Pro\\DCIM\\Camera')\n",
    "\n",
    "print(f\"Listing subdirectories in: {photos_root}\\n\")\n",
    "\n",
    "# Check if path exists\n",
    "if not photos_root.exists():\n",
    "    print(f\"ERROR: Path does not exist: {photos_root}\")\n",
    "elif not photos_root.is_dir():\n",
    "    print(f\"ERROR: Path is not a directory: {photos_root}\")\n",
    "else:\n",
    "    # List all subdirectories\n",
    "    subdirs = [d for d in photos_root.iterdir() if d.is_dir()]\n",
    "    \n",
    "    print(f\"Found {len(subdirs)} subdirectories:\\n\")\n",
    "    \n",
    "    for subdir in sorted(subdirs):\n",
    "        print(f\"  ðŸ“ {subdir.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at files in a specific subdirectory\n",
    "subdir = \"2025/01/01\"\n",
    "# subdir = \"2025/12\"\n",
    "target_dir = photos_root / subdir\n",
    "\n",
    "print(f\"Examining directory: {target_dir}\")\n",
    "print(f\"Subdirectory (relative): {subdir}\")\n",
    "\n",
    "if target_dir.exists() and target_dir.is_dir():\n",
    "    file_list = [f for f in target_dir.iterdir() if f.is_file()]\n",
    "    print(f\"Number of files: {len(file_list)}\")\n",
    "\n",
    "    # Show first 5 files as examples\n",
    "    print(f\"\\nFirst 5 files:\")\n",
    "    for f in file_list[:5]:\n",
    "        print(f\"  {f.name}\")\n",
    "else:\n",
    "    print(\"ERROR: Directory does not exist or is not accessible\")\n",
    "\n",
    "# Create a list to hold file metadata\n",
    "file_data = []\n",
    "\n",
    "if target_dir.exists() and target_dir.is_dir():\n",
    "    for file_path in target_dir.iterdir():\n",
    "        if file_path.is_file():\n",
    "            # Get file stats\n",
    "            stats = file_path.stat()\n",
    "\n",
    "            # Get relative subdirectory path from photos_root_original\n",
    "            rel_subdir = file_path.parent.relative_to(photos_root)\n",
    "\n",
    "            file_data.append({\n",
    "                'filename': file_path.name,\n",
    "                'extension': file_path.suffix.lower(),\n",
    "                'subdirectory': str(rel_subdir),\n",
    "                'created': datetime.datetime.fromtimestamp(stats.st_ctime),\n",
    "                'modified': datetime.datetime.fromtimestamp(stats.st_mtime),\n",
    "                'size_bytes': stats.st_size\n",
    "            })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(file_data)\n",
    "\n",
    "print(f\"\\nCreated DataFrame with {len(df)} files\")\n",
    "print(f\"\\nFirst 10 rows:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Filename Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze filename patterns to find common prefixes\n",
    "from collections import defaultdict\n",
    "\n",
    "# Group files by their base name (without extension)\n",
    "file_groups = defaultdict(list)\n",
    "\n",
    "if target_dir.exists() and target_dir.is_dir():\n",
    "    for file_path in target_dir.iterdir():\n",
    "        if file_path.is_file():\n",
    "            # Get the stem (filename without extension)\n",
    "            base_name = file_path.stem\n",
    "            file_groups[base_name].append(file_path)\n",
    "\n",
    "# Show some examples of grouped files\n",
    "print(f\"Total unique base names: {len(file_groups)}\\n\")\n",
    "print(\"Examples of files with same base name:\\n\")\n",
    "\n",
    "count = 0\n",
    "for base_name, files in sorted(file_groups.items()):\n",
    "    if len(files) > 1:  # Only show groups with multiple files\n",
    "        print(f\"{base_name}:\")\n",
    "        for f in files:\n",
    "            print(f\"  - {f.name}\")\n",
    "        print()\n",
    "        count += 1\n",
    "        if count >= 5:  # Show first 5 examples\n",
    "            break\n",
    "\n",
    "# Count how many base names have multiple files\n",
    "multi_file_groups = {k: v for k, v in file_groups.items() if len(v) > 1}\n",
    "print(f\"\\nBase names with multiple files: {len(multi_file_groups)}\")\n",
    "print(f\"Base names with single file: {len(file_groups) - len(multi_file_groups)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Grouped DataFrame (One Row Per Base Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with one row per base name (grouped files)\n",
    "grouped_data = []\n",
    "\n",
    "for base_name, files in sorted(file_groups.items()):\n",
    "    # Get all extensions for this base name\n",
    "    extensions = [f.suffix.lower() for f in files]\n",
    "\n",
    "    # Get the earliest created and latest modified dates across all variants\n",
    "    created_dates = [datetime.datetime.fromtimestamp(f.stat().st_ctime) for f in files]\n",
    "    modified_dates = [datetime.datetime.fromtimestamp(f.stat().st_mtime) for f in files]\n",
    "\n",
    "    # Get total size of all variants\n",
    "    total_size = sum(f.stat().st_size for f in files)\n",
    "\n",
    "    # Get relative subdirectory (same for all files in group)\n",
    "    rel_subdir = files[0].parent.relative_to(photos_root)\n",
    "\n",
    "    grouped_data.append({\n",
    "        'base_name': base_name,\n",
    "        'file_count': len(files),\n",
    "        'extensions': ', '.join(sorted(extensions)),\n",
    "        'subdirectory': str(rel_subdir),\n",
    "        'created': min(created_dates),\n",
    "        'modified': max(modified_dates),\n",
    "        'total_size_bytes': total_size\n",
    "    })\n",
    "\n",
    "# Create grouped DataFrame\n",
    "df_grouped = pd.DataFrame(grouped_data)\n",
    "\n",
    "print(f\"Created grouped DataFrame with {len(df_grouped)} unique base names\")\n",
    "print(f\"\\nFirst 10 rows:\")\n",
    "df_grouped.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more sophisticated grouping that handles various naming patterns\n",
    "# Patterns to handle:\n",
    "# 1. Standard: \"2025-01-01_00-28-40.jpg\" and \"2025-01-01_00-28-40_001.jpg\"\n",
    "# 2. Google Pixel: \"PXL_20251210_200246684.RAW-01.COVER.jpg\" and \"PXL_20251210_200246684.RAW-02.ORIGINAL.dng\"\n",
    "# 3. XMP sidecars: \"filename.jpg.xmp\"\n",
    "\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "image_groups = defaultdict(list)\n",
    "\n",
    "if target_dir.exists() and target_dir.is_dir():\n",
    "    for file_path in target_dir.iterdir():\n",
    "        if file_path.is_file():\n",
    "            filename = file_path.name\n",
    "\n",
    "            # Strategy: Find the base name by detecting common patterns\n",
    "            base_name = filename\n",
    "\n",
    "            # Pattern 1: Google Pixel RAW files (PXL_timestamp.RAW-##.TYPE.ext)\n",
    "            # Extract everything before \".RAW-\"\n",
    "            if '.RAW-' in filename.upper():\n",
    "                base_name = re.split(r'\\.RAW-', filename, flags=re.IGNORECASE)[0]\n",
    "            else:\n",
    "                # Pattern 2: Standard files - remove all extensions first\n",
    "                name_without_ext = filename\n",
    "                while '.' in name_without_ext:\n",
    "                    name_without_ext = Path(name_without_ext).stem\n",
    "\n",
    "                # Pattern 3: Check for numeric suffix like _001, _002 at the end\n",
    "                match = re.match(r'^(.+?)(_\\d+)?$', name_without_ext)\n",
    "                if match:\n",
    "                    base_name = match.group(1)\n",
    "                else:\n",
    "                    base_name = name_without_ext\n",
    "\n",
    "            # Calculate the suffix (everything after base_name)\n",
    "            suffix = filename[len(base_name):]\n",
    "\n",
    "            image_groups[base_name].append({\n",
    "                'file_path': file_path,\n",
    "                'suffix': suffix\n",
    "            })\n",
    "\n",
    "# Create DataFrame with one row per image\n",
    "image_data = []\n",
    "\n",
    "for base_name, files_info in sorted(image_groups.items()):\n",
    "    # Get all suffixes\n",
    "    suffixes = [info['suffix'] for info in files_info]\n",
    "\n",
    "    # Get file paths\n",
    "    file_paths = [info['file_path'] for info in files_info]\n",
    "\n",
    "    # Get stats from all files\n",
    "    created_dates = [datetime.datetime.fromtimestamp(f.stat().st_ctime) for f in file_paths]\n",
    "    modified_dates = [datetime.datetime.fromtimestamp(f.stat().st_mtime) for f in file_paths]\n",
    "    total_size = sum(f.stat().st_size for f in file_paths)\n",
    "\n",
    "    # Get relative subdirectory\n",
    "    rel_subdir = file_paths[0].parent.relative_to(photos_root)\n",
    "\n",
    "    image_data.append({\n",
    "        'base_name': base_name,\n",
    "        'suffixes': suffixes,\n",
    "        'file_count': len(suffixes),\n",
    "        'subdirectory': str(rel_subdir),\n",
    "        'created': min(created_dates),\n",
    "        'modified': max(modified_dates),\n",
    "        'total_size_bytes': total_size\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df_images = pd.DataFrame(image_data)\n",
    "\n",
    "print(f\"Created image DataFrame with {len(df_images)} unique images\")\n",
    "print(f\"\\nFirst 10 rows:\")\n",
    "df_images.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
